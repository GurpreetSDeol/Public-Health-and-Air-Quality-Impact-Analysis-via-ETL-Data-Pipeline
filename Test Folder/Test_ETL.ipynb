{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "import pandas as pd \n",
    "import time\n",
    "from json_flatten import flatten\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all credentials \n",
    "with open(rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data_ETL\\config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "OW_api_key = config['OW_api_key']\n",
    "db_name = config['db_name']\n",
    "db_user = config['user']\n",
    "db_pass = config['password']\n",
    "db_host = config['host']\n",
    "db_port = config['port']\n",
    "\n",
    "#Load data containing cities for the API\n",
    "json_save_path = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data_ETL\\Final_city_data.json'\n",
    "with open(json_save_path) as f:\n",
    "    city_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather and Pollution API requests\n",
    "OW_weather_data = []\n",
    "OW_pollution_data = []\n",
    "delay = 2\n",
    "\n",
    "city_data = city_data[:2]\n",
    "\n",
    "for city in city_data:\n",
    "\n",
    "     #Load the longitude and latitude data for each city\n",
    "     lat = city['latitude']\n",
    "     lon = city['longitude']\n",
    "     city_id = city['city_id']\n",
    "\n",
    "     #Fetch the current weather data \n",
    "     OW_weather_url = f'https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OW_api_key}'\n",
    "     response_weather = requests.get(OW_weather_url)\n",
    "     weather_data = response_weather.json()\n",
    "     weather_data['latitude'] = lat\n",
    "     weather_data['longitude'] = lon\n",
    "     weather_data['city_id'] = city_id\n",
    "     OW_weather_data.append(weather_data)\n",
    "     time.sleep(delay)\n",
    "\n",
    "     #Fetch the current air quality data\n",
    "     OW_pollution_url = f'http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={OW_api_key}'\n",
    "     response_pollution = requests.get(OW_pollution_url)\n",
    "     pollution_data = response_pollution.json()\n",
    "     pollution_data['latitude'] = lat\n",
    "     pollution_data['longitude'] = lon\n",
    "     pollution_data['city_id'] = city_id\n",
    "     OW_pollution_data.append(pollution_data)\n",
    "     time.sleep(delay)\n",
    "    \n",
    "\n",
    "#Save each file \n",
    "OW_raw_weather_filepath = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Raw\\OW_raw_weather_data.json'\n",
    "with open(OW_raw_weather_filepath, 'w') as f:\n",
    "    json.dump(OW_weather_data, f, indent=4)\n",
    "\n",
    "OW_raw_pollution_filepath = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Raw\\OW_raw_pollution_data.json'\n",
    "with open(OW_raw_pollution_filepath, 'w') as f:\n",
    "    json.dump(OW_pollution_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OW_raw_weather_filepath = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Raw\\OW_raw_weather_data.json'\n",
    "\n",
    "with open(OW_raw_weather_filepath) as f:\n",
    "    OW_weather_data = json.load(f)\n",
    "\n",
    "OW_raw_pollution_filepath = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Raw\\OW_raw_pollution_data.json'\n",
    "with open(OW_raw_pollution_filepath) as f:\n",
    "    OW_pollution_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for handling column names and datatypes\n",
    "def rename_and_convert_columns(df, column_map):\n",
    "    \"\"\"\n",
    "    Renames columns in a DataFrame and converts them to the specified data types.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_map (dict): A dictionary where keys are original column names and values are tuples of \n",
    "                           the new column name and data type (e.g., ('new_name', 'dtype')).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with renamed columns and converted data types.\n",
    "    \"\"\"\n",
    "    for original_col, (new_col, dtype) in column_map.items():\n",
    "        if original_col in df.columns:\n",
    "            df.rename(columns={original_col: new_col}, inplace=True)\n",
    "            \n",
    "            # Convert to appropriate data type\n",
    "            if dtype == 'float':\n",
    "                df[new_col] = pd.to_numeric(df[new_col], errors='coerce')\n",
    "            elif dtype == 'int':\n",
    "                df[new_col] = pd.to_numeric(df[new_col], errors='coerce', downcast='integer')\n",
    "            elif dtype == 'datetime':\n",
    "                # Explicitly cast to numeric before converting to datetime\n",
    "                df[new_col] = pd.to_datetime(pd.to_numeric(df[new_col], errors='coerce'), unit='s', errors='coerce')\n",
    "                df[new_col] = df[new_col].dt.round('h')\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_local_time(datetime_str, lat, lon):\n",
    "    \"\"\"\n",
    "    Calculates the local time based on latitude, longitude, and UK local datetime.\n",
    "\n",
    "    Parameters:\n",
    "    datetime_str (str): The datetime in UK local time as a string.\n",
    "    lat (float): Latitude of the location.\n",
    "    lon (float): Longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    pd.Timestamp: The local time as a Pandas Timestamp object.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Parse the UK local datetime string into a datetime object\n",
    "    uk_datetime = pd.to_datetime(datetime_str)\n",
    "\n",
    "    # Initialize TimezoneFinder\n",
    "    tf = TimezoneFinder()\n",
    "\n",
    "    # Find timezone based on latitude and longitude\n",
    "    timezone_str = tf.timezone_at(lng=lon, lat=lat)\n",
    "\n",
    "    # Get the timezone\n",
    "    timezone = pytz.timezone(timezone_str)\n",
    "    # Convert the UK local time to local time in the identified timezone\n",
    "    local_time = pd.Timestamp(uk_datetime).tz_localize(pytz.timezone('Europe/London')).astimezone(timezone)\n",
    "    return local_time.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names, format and dtypes for the weather data \n",
    "flattened_weather_data = [flatten(item) for item in OW_weather_data]\n",
    "weather_data_df = pd.DataFrame(flattened_weather_data)\n",
    "\n",
    "        # Define a mapping for column renaming and data type conversions\n",
    "weather_column_map = {\n",
    "    'city_id$int': ('city_id', 'int'),\n",
    "    'longitude$float': ('longitude', 'float'),\n",
    "    'latitude$float': ('latitude', 'float'),\n",
    "    'weather.[0].id$int': ('weather_id', 'int'),\n",
    "    'weather.[0].main': ('weather_main', 'str'),\n",
    "    'weather.[0].description': ('weather_description', 'str'),\n",
    "    'weather.[0].icon': ('weather_icon', 'str'),\n",
    "    'main.temp$float': ('temperature', 'float'),\n",
    "    'main.feels_like$float': ('feels_like', 'float'),\n",
    "    'main.temp_min$float': ('temp_min', 'float'),\n",
    "    'main.temp_max$float': ('temp_max', 'float'),\n",
    "    'main.pressure$int': ('pressure', 'int'),\n",
    "    'main.humidity$int': ('humidity', 'int'),\n",
    "    'main.sea_level$int': ('sea_level', 'int'),\n",
    "    'main.grnd_level$int': ('grnd_level', 'int'),\n",
    "    'visibility$int': ('visibility', 'int'),\n",
    "    'wind.speed$float': ('wind_speed', 'float'),\n",
    "    'wind.deg$int': ('wind_deg', 'int'),\n",
    "    'clouds.all$int': ('clouds_all', 'int'),\n",
    "    'dt$int': ('date_time', 'datetime'),\n",
    "    'sys.type$int': ('sys_type', 'int'),\n",
    "    'sys.id$int': ('sys_id', 'int'),\n",
    "    'sys.country': ('sys_country', 'str'),\n",
    "    'sys.sunrise$int': ('sunrise', 'datetime'),\n",
    "    'sys.sunset$int': ('sunset', 'datetime'),\n",
    "    'timezone$int': ('timezone', 'int'),\n",
    "    'id$int': ('id', 'int'),\n",
    "    'name': ('name', 'str'),\n",
    "    'cod$int': ('cod', 'int'),\n",
    "    'wind.gust$float': ('wind_gust', 'float'),\n",
    "    'rain.1h$float': ('rain_1h', 'float')\n",
    "}\n",
    "\n",
    "weather_data_df = rename_and_convert_columns(weather_data_df, weather_column_map)\n",
    "weather_data_df['local_time'] = weather_data_df.apply(\n",
    "    lambda row: calculate_local_time(row['date_time'], row['latitude'], row['longitude']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "    # Define the columns to keep\n",
    "required_columns = [\n",
    "    'city_id',  'date_time', 'local_time','temperature', 'feels_like', 'temp_min', 'temp_max',\n",
    "    'pressure', 'humidity', 'visibility', 'wind_speed', 'wind_deg', 'clouds_all',\n",
    "    'weather_main', 'weather_description', 'weather_icon', 'sunrise', 'sunset'\n",
    "]\n",
    "\n",
    "    # Filter DataFrame to keep only the required columns\n",
    "weather_data_df = weather_data_df[required_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   city_id           date_time          local_time  temperature  feels_like  \\\n",
      "0        1 2024-09-02 16:00:00 2024-09-03 00:00:00       300.70      304.41   \n",
      "1        2 2024-09-02 16:00:00 2024-09-02 20:30:00       303.21      310.21   \n",
      "\n",
      "   temp_min  temp_max  pressure  humidity  visibility  wind_speed  wind_deg  \\\n",
      "0    300.12    301.28      1009        82       10000        6.17       210   \n",
      "1    303.21    303.21      1007        84        4000        2.06        80   \n",
      "\n",
      "   clouds_all weather_main weather_description weather_icon  \\\n",
      "0          20       Clouds          few clouds          02n   \n",
      "1          40         Mist                mist          50n   \n",
      "\n",
      "              sunrise              sunset  \n",
      "0 2024-09-02 20:00:00 2024-09-03 09:00:00  \n",
      "1 2024-09-02 00:00:00 2024-09-02 13:00:00  \n"
     ]
    }
   ],
   "source": [
    "# Change the column names, format and dtypes for the pollution data \n",
    "flattened_pollution_data = [flatten(item) for item in OW_pollution_data]\n",
    "\n",
    "\n",
    "\n",
    "pollution_data_df = pd.DataFrame(flattened_pollution_data)\n",
    "\n",
    "        # Define a mapping for column renaming and data type conversions\n",
    "pollution_column_map = {\n",
    "    'city_id$int': ('city_id', 'int'),\n",
    "    'list.[0].dt$int': ('date_time', 'datetime'),\n",
    "    'longitude$float': ('longitude', 'float'),\n",
    "    'latitude$float': ('latitude', 'float'),\n",
    "    'list.[0].main.aqi$int': ('aqi', 'int'),\n",
    "    'list.[0].components.co$float': ('co', 'float'),\n",
    "    'list.[0].components.no$int': ('no', 'floT'),\n",
    "    'list.[0].components.no2$float': ('no2', 'float'),\n",
    "    'list.[0].components.o3$float': ('o3', 'float'),\n",
    "    'list.[0].components.so2$float': ('so2', 'float'),\n",
    "    'list.[0].components.pm2_5$float': ('pm2_5', 'float'),\n",
    "    'list.[0].components.pm10$float': ('pm10', 'float'),\n",
    "    'list.[0].components.nh3$float': ('nh3', 'float')\n",
    "}\n",
    "\n",
    "        # Rename columns and convert data types\n",
    "pollution_data_df = rename_and_convert_columns(pollution_data_df, pollution_column_map)\n",
    "\n",
    "\n",
    "        # Define the columns to keep\n",
    "required_pollution_columns = [col for col in pollution_column_map.values() if col[0] in pollution_data_df.columns]\n",
    "\n",
    "\n",
    "        # Filter DataFrame to keep only the required columns\n",
    "pollution_data_df = pollution_data_df[[col[0] for col in required_pollution_columns]]\n",
    "pollution_data_df['local_time'] = pollution_data_df.apply(\n",
    "    lambda row: calculate_local_time(row['date_time'], row['latitude'], row['longitude']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "pollution_data_df=pollution_data_df.drop(columns=['latitude','longitude'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   city_id           date_time          local_time  temperature  feels_like  \\\n",
      "0        1 2024-09-02 16:00:00 2024-09-03 00:00:00       300.70      304.41   \n",
      "1        2 2024-09-02 16:00:00 2024-09-02 20:30:00       303.21      310.21   \n",
      "\n",
      "   temp_min  temp_max  pressure  humidity  visibility  wind_speed  wind_deg  \\\n",
      "0    300.12    301.28      1009        82       10000        6.17       210   \n",
      "1    303.21    303.21      1007        84        4000        2.06        80   \n",
      "\n",
      "   clouds_all weather_main weather_description weather_icon  \\\n",
      "0          20       Clouds          few clouds          02n   \n",
      "1          40         Mist                mist          50n   \n",
      "\n",
      "              sunrise              sunset  \n",
      "0 2024-09-02 20:00:00 2024-09-03 09:00:00  \n",
      "1 2024-09-02 00:00:00 2024-09-02 13:00:00  \n",
      "   city_id           date_time  aqi       co   no    no2     o3    so2  pm2_5  \\\n",
      "0        1 2024-09-02 16:00:00    2   310.42  NaN  43.18   0.05  18.60   7.03   \n",
      "1        2 2024-09-02 16:00:00    4  1081.47    0  28.79  37.55  26.23  56.07   \n",
      "\n",
      "    pm10   nh3          local_time  \n",
      "0   8.78  0.98 2024-09-03 00:00:00  \n",
      "1  95.19  9.37 2024-09-02 20:30:00  \n"
     ]
    }
   ],
   "source": [
    "print(weather_data_df.head())\n",
    "print(pollution_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurpr\\AppData\\Local\\Temp\\ipykernel_9424\\4090169382.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  weather_data_df.to_sql('weather', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\gurpr\\AppData\\Local\\Temp\\ipykernel_9424\\4090169382.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pollution_data_df.to_sql('pollution', con=engine, if_exists='append', index=False)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_pass}\\\n",
    "@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "weather_csv_path = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Failed_to_Upload\\Weather\\Weather_{timestamp}.csv'\n",
    "pollution_csv_path = rf'C:\\Users\\gurpr\\OneDrive\\Documents\\New Projects\\Weather and Pollution\\Data\\Failed_to_Upload\\Pollution\\pollution_{timestamp}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "def test_connection():\n",
    "    try:\n",
    "        # Establish a connection using a context manager\n",
    "        with engine.connect() as connection:\n",
    "            print(\"Connection successful!\")\n",
    "            # Execute a simple query to verify connection using text\n",
    "            result = connection.execute(text(\"SELECT 1\"))\n",
    "            print(\"Query result:\", result.scalar())\n",
    "    except Exception as e:\n",
    "        print(\"Connection failed:\", e)\n",
    "\n",
    "\n",
    "try:\n",
    "    weather_data_df.to_sql('weather', con=engine, if_exists='append', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    weather_data_df.to_csv(weather_csv_path, index=False)\n",
    "\n",
    "try:\n",
    "    pollution_data_df.to_sql('pollution', con=engine, if_exists='append', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    pollution_data_df.to_csv(pollution_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(dbname=db_name, user=db_user, password=db_pass, host=db_host, port=db_port)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def bulk_insert_pandas(df, table_name):\n",
    "    columns = ', '.join(df.columns)\n",
    "    values = ', '.join([f\"%({col})s\" for col in df.columns])\n",
    "    sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({values})\"\n",
    "    \n",
    "    # Convert DataFrame to list of dicts\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "    # Execute the bulk insert\n",
    "    execute_batch(cursor, sql, data)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# Upload data\n",
    "try:\n",
    "    bulk_insert_pandas(weather_data_df, 'weather')\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading weather data: {e}\")\n",
    "    weather_data_df.to_csv(weather_csv_path, index=False)\n",
    "\n",
    "try:\n",
    "    bulk_insert_pandas(pollution_data_df, 'pollution')\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading pollution data: {e}\")\n",
    "    pollution_data_df.to_csv(pollution_csv_path, index=False)\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
